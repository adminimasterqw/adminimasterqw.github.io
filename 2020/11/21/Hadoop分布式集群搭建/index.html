<!-- build time:Sat Nov 21 2020 19:18:53 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Andrésen" href="https://methiony.work/rss.xml"><link rel="alternate" type="application/atom+xml" title="Andrésen" href="https://methiony.work/atom.xml"><link rel="alternate" type="application/json" title="Andrésen" href="https://methiony.work/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.4"><link rel="canonical" href="https://methiony.work/2020/11/21/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"><title>| Methiony Shoka = Andrésen</title><meta name="generator" content="Hexo 5.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline"></h1><div class="meta"><span class="item" title="创建时间：2020-11-21 18:41:39"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2020-11-21T18:41:39+08:00">2020-11-21</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>8.3k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>8 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Methiony Shoka</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1gipexj2jgzj20zk0m8b09.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1gipeu1usa7j20zk0m8b29.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giclxxcb6rj20zk0m8b29.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giclx29mstj20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1gicli3sbvtj20zk0m8x6p.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1gipex2cdtbj20zk0m8x6p.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://methiony.work/2020/11/21/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Andrésen"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Andrésen"></span><div class="body md" itemprop="articleBody"><p>title: Zookeeper 基础知识<br>date: 2020-11-21 17:40:19<br>tags:</p><h1 id="hadoop分布式集群搭建"><a class="anchor" href="#hadoop分布式集群搭建">#</a> Hadoop 分布式集群搭建</h1><h2 id="1-环境准备"><a class="anchor" href="#1-环境准备">#</a> 1、环境准备</h2><p>首先准备了三台虚拟机（为了避免环境的干扰，都是新建的虚拟机），分别分配了 1G 内存、20G 硬盘、2 核 CPU，然后安装了 CentOS-7-x86_64-DVD-2009 操作系统，并配置了静态 IP 地址。</p><p><strong>集群规划：</strong></p><p>搭建的是 NameNode 与 ResourceManager 单节点架构。节点具体分别如下：</p><table><thead><tr><th>服务器名称</th><th>IP 地址</th><th>NameNode</th><th>SecondaryNameNode</th><th>dataNode</th><th>ResourceManager</th><th>NodeManager</th></tr></thead><tbody><tr><td>master</td><td>192.168.2.128</td><td>是</td><td>是</td><td>是</td><td>是</td><td>是</td></tr><tr><td>node01</td><td>192.168.2.129</td><td>否</td><td>否</td><td>是</td><td>否</td><td>是</td></tr><tr><td>node02</td><td>192.168.2.130</td><td>否</td><td>否</td><td>是</td><td>否</td><td>是</td></tr></tbody></table><p><strong>文件系统核心模块：</strong><br>NameNode：集群当中的主节点，主要用于管理集群当中的各种数据<br>secondaryNameNode：主要能用于 hadoop 当中元数据信息的辅助管理<br>DataNode：集群当中的从节点，主要用于存储集群当中的各种数据<br><strong>数据计算核心模块：</strong><br>ResourceManager：接收用户的计算请求任务，并负责集群的资源分配<br>NodeManager：负责执行主节点 APPmaster 分配的任务</p><h2 id="2-环境配置"><a class="anchor" href="#2-环境配置">#</a> 2、环境配置</h2><p>1、关闭防火墙</p><pre><code>[root@localhost ~]# systemctl stop firewalld.service 
#关闭防火墙
[root@localhost ~]# systemctl disable firewalld.service 
#禁止开机启动
[root@localhost ~]*# firewall-cmd --state 
#查看防火墙状态
not running
</code></pre><p>2、设置主机名称</p><pre><code>#在192.168.2.128机器上执行* 
[root@localhost ~]*# hostnamectl set-hostname  master

*#在192.168.2.129机器上执行* 
[root@localhost ~]*# hostnamectl set-hostname node01

*#在192.168.2.130机器上执行* 
[root@localhost ~]*# hostnamectl set-hostname node02
</code></pre><p>3、三台虚拟机重启</p><pre><code>#在192.168.2.128机器上执行* 
[root@localhost ~]# reboot

*#在192.168.2.129机器上执行* 
[root@localhost ~]# reboot

*#在192.168.2.130机器上执行* 
[root@localhost ~]# reboot
</code></pre><p>4、关闭 selinux</p><p>进入到 /etc/selinux/config 文件，将 SELINUX=enforcing 改为 SELINUX=disabled。</p><pre><code>[root@master ~]#  vi  /etc/selinux/config
</code></pre><p>5、设置免密登录</p><p>首先在 master 主机上使用 ssh-keygen 命令来生成秘钥，一路按回车键</p><pre><code>[root@master~]#ssh-keygen -t rsa
</code></pre><p>通过 ssh-copy-id 命令设置免密钥登录<br>其中，master、node01、node02 表示需要设置免密登录的服务器地址。</p><figure class="highlight powershell"><figcaption data-lang="PowerShell"></figcaption><table><tr><td data-num="1"></td><td><pre>ssh<span class="token operator">-</span><span class="token function">copy-id</span> <span class="token operator">-</span>i ~<span class="token operator">/</span><span class="token punctuation">.</span>ssh<span class="token operator">/</span>id_rsa<span class="token punctuation">.</span>pub root@192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>2<span class="token punctuation">.</span>128</pre></td></tr><tr><td data-num="2"></td><td><pre>ssh<span class="token operator">-</span><span class="token function">copy-id</span> <span class="token operator">-</span>i ~<span class="token operator">/</span><span class="token punctuation">.</span>ssh<span class="token operator">/</span>id_rsa<span class="token punctuation">.</span>pub root@192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>2<span class="token punctuation">.</span>129</pre></td></tr><tr><td data-num="3"></td><td><pre>ssh<span class="token operator">-</span><span class="token function">copy-id</span> <span class="token operator">-</span>i ~<span class="token operator">/</span><span class="token punctuation">.</span>ssh<span class="token operator">/</span>id_rsa<span class="token punctuation">.</span>pub root@192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>2<span class="token punctuation">.</span>130</pre></td></tr></table></figure><h2 id="3-安装jdk"><a class="anchor" href="#3-安装jdk">#</a> 3、安装 JDK</h2><p>在 master 中新建目录 /opt/bigdata/, 此目录下存放 hadoop 大数据所需要的环境包 (jdk 和 hadoop).</p><pre><code>[root@master~]#mkdir /opt/bigdata
</code></pre><p>1、把下载好的 jdk-8u271-linux-x64.tar.gz 包和 hadoop-3.1.4.tar.gz 上传至 master 主机中，JDK 是安装 Hadoop 的基础环境，所以需要优先安装好 JDK 环境</p><p>2、解压 jdk 并复制到 /opt/bigdata 目录下</p><pre><code>[root@master~]#tar -zxvf jdk-8u271-linux-x64.tar.gz -C
/opt/bigdata
</code></pre><p>3、配置环境变量 (/etc/profile)</p><pre><code>[root@master~]#vi /etc/profile
</code></pre><p>添加</p><pre><code>export JAVA_HOME=/export/servers/jdk1.8.0_271 
export PATH=:$JAVA_HOME/bin:$PATH
</code></pre><p>执行下面命令，让环境变量生效</p><pre><code>[root@master~]#source/etc/profile
</code></pre><p>验证 jdk 安装是否成功</p><pre><code>[root@master~]#java -version
</code></pre><h2 id="4-安装hadoop"><a class="anchor" href="#4-安装hadoop">#</a> 4、安装 hadoop</h2><p>1、解压 hadoop，并把解压文件复制到 /opt/bigdata 目录中</p><pre><code>[root@master~]#tar -zxvf hadoop-3.1.4.tar.gz -C /opt/bigdata
</code></pre><p>2、配置环境变量 (/etc/profile)</p><pre><code>[root@master~]#vi /etc/profile
</code></pre><p>添加</p><pre><code>export HADOOP_HOME=/export/servers/hadoop-3.1.4
export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
</code></pre><p>执行下面命令，让环境变量生效</p><pre><code>[root@master~]#source/etc/profile
</code></pre><p>3、修改 hadoop 的配置文件</p><p>进入需改文件的目录</p><pre><code>[root@master~]#cd /opt/bigdata/hadoop-3.1.4/etc/hadoop
</code></pre><p>需要修改的文件</p><pre><code>core-site.xml	hadoop-env.sh	hdfs-site.xml	
mapred-env.sh	mapred-site.xml	yarn-site.xml
</code></pre><p>首先，修改 core-site.xml 文件，在 &lt;configuration&gt; 元素中添加相应的配置文件</p><pre><code>&lt;configuration&gt; 	
	&lt;!--  指定集群的文件系统类型:分布式文件系统 --&gt; 	
	&lt;property&gt; 		
		&lt;name&gt;fs.default.name&lt;/name&gt; 		
	&lt;!--因为前面已经配置了服务器名称映射IP，所以这里可以使用服务器名称进行配置--&gt; 		
		&lt;value&gt;hdfs://master:8020&lt;/value&gt; 	
	&lt;/property&gt; 
	&lt;!--  指定临时文件存储目录 --&gt; 	
	&lt;property&gt; 
		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt; 					     &lt;value&gt;/opt/bigdata/hadoop3.1.4/hadoopDatas/tempDatas&lt;/value&gt; 	
	&lt;/property&gt;
	&lt;!--  缓冲区大小，实际工作中根据服务器性能动态调整 --&gt; 	
	&lt;property&gt; 		
		&lt;name&gt;io.file.buffer.size&lt;/name&gt; 		
		&lt;value&gt;4096&lt;/value&gt; 	
	&lt;/property&gt;  	
	&lt;!--  开启hdfs的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 --&gt; 	
	&lt;property&gt; 
		&lt;name&gt;fs.trash.interval&lt;/name&gt; 
		&lt;value&gt;10080&lt;/value&gt; 	
	&lt;/property&gt; 
&lt;/configuration&gt;
</code></pre><p>其次，修改 hdfs-site.xml 文件，在 &lt;configuration&gt; 元素中添加相应的配置文件</p><pre><code>&lt;configuration&gt;
	 &lt;property&gt;
			&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
			&lt;value&gt;master:50090&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- 指定namenode的访问地址和端口 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
		&lt;value&gt;master:50070&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 指定namenode元数据的存放位置 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
		&lt;value&gt;file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/namenodeDatas,file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/namenodeDatas2&lt;/value&gt;
	&lt;/property&gt;
	&lt;!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/datanodeDatas,file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/datanodeDatas2&lt;/value&gt;
	&lt;/property&gt;
	
	&lt;!-- 指定namenode日志文件的存放目录 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt;
		&lt;value&gt;file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/nn/edits&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
		&lt;value&gt;file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/snn/name&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt;
		&lt;value&gt;file:///opt/bigdata/hadoop-3.1.4/hadoopDatas/dfs/snn/edits&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 文件切片的副本个数--&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;3&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- 设置HDFS的文件权限--&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- 设置一个文件切片的大小：128M--&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.blocksize&lt;/name&gt;
		&lt;value&gt;134217728&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>然后，修改 yarn-site.xml 文件，在 &lt;configuration&gt; 元素中添加相应的配置文件</p><pre><code>&lt;configuration&gt;
	&lt;!-- 配置yarn主节点的位置 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
		&lt;value&gt;master&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
	&lt;/property&gt;
	
	&lt;!-- 开启日志聚合功能 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 设置聚合日志在hdfs上的保存时间 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
		&lt;value&gt;604800&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 设置yarn集群的内存分配方案 --&gt;
	&lt;property&gt;    
		&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;    
		&lt;value&gt;20480&lt;/value&gt;
	&lt;/property&gt;

	&lt;property&gt;  
        	 &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
         	&lt;value&gt;2048&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
		&lt;value&gt;2.1&lt;/value&gt;
	&lt;/property&gt;

&lt;/configuration&gt;
</code></pre><p>然后，修改 mapred-site.xml 文件，在 &lt;configuration&gt; 元素中添加相应的配置文件</p><pre><code>&lt;configuration&gt;
	&lt;!-- 开启MapReduce小任务模式 --&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	
	&lt;!-- 设置历史任务的主机和端口 --&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
		&lt;value&gt;master:10020&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- 设置网页访问历史任务的主机和端口 --&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
		&lt;value&gt;master:19888&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>然后，<span class="exturl" data-url="aHR0cDovL3huLS1oYWRvb3AtZW52LXl1NnA1NHIwbmE1NjF0LnNo">分别修改 hadoop-env.sh</span>、mapred-env.sh 文件</p><pre><code>[root@master hadoop]#vi hadoop-env.sh
</code></pre><pre><code>[root@master hadoop]#vi mapred-env.sh
</code></pre><p>添加 Java 环境变量</p><pre><code>export JAVA_HOME=/opt/bigdata/jdk1.8.0_271
</code></pre><p>最后，修改集群配置文件 slaves（没有则新建）</p><pre><code>[root@master hadoop]#vi slaves
</code></pre><p>添加</p><pre><code>master
node01
node02
</code></pre><p>4、创建 hadoop 需要的文件目录</p><figure class="highlight powershell"><figcaption data-lang="PowerShell"></figcaption><table><tr><td data-num="1"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>tempDatas</pre></td></tr><tr><td data-num="2"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>namenodeDatas</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>namenodeDatas2</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>datanodeDatas</pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>datanodeDatas2 </pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>nn<span class="token operator">/</span>edits</pre></td></tr><tr><td data-num="11"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>snn<span class="token operator">/</span>name</pre></td></tr><tr><td data-num="12"></td><td><pre>mkdir <span class="token operator">-</span>p <span class="token operator">/</span>opt<span class="token operator">/</span>bigdata<span class="token operator">/</span>hadoop<span class="token operator">-</span>3<span class="token punctuation">.</span>1<span class="token punctuation">.</span>4<span class="token operator">/</span>hadoopDatas<span class="token operator">/</span>dfs<span class="token operator">/</span>snn<span class="token operator">/</span>edits</pre></td></tr></table></figure><h2 id="5-分发jdk-hadoop到node01-node02"><a class="anchor" href="#5-分发jdk-hadoop到node01-node02">#</a> 5、分发 jdk、hadoop 到 node01、node02</h2><p>首先进入 jdk、hadoop 的安装目录</p><pre><code>[root@master hadoop]#cd /opt/bigdata
</code></pre><p>然后分发 jdk，分别到 node01、node02</p><pre><code>scp -r jdk1.8.0_271 node01:$PWD 
scp -r jdk1.8.0_271 node02:$PWD
</code></pre><p>分发 Hadoop，分别到 node01、node02</p><pre><code>scp -r hadoop-3.1.4 node01:$PWD 
scp -r hadoop-3.1.4 node02:$PWD
</code></pre><p>分发 master 已经配置好的环境变量文件到 node01、node02 (需要进入到根目录)</p><pre><code>scp -r /etc/profile root@node01:/etc/profile 
scp -r /etc/profile root@node02:/etc/profile
</code></pre><p>执行下面命令，让环境变量生效</p><pre><code>[root@node01~]#source/etc/profile
[root@node02~]#source/etc/profile
</code></pre><p>检查是否生效</p><pre><code>[root@node01~]#java -version
[root@node02~]#java -version
</code></pre><h2 id="6-启动集群"><a class="anchor" href="#6-启动集群">#</a> 6、启动集群</h2><p>首先，在 master 主机进行格式化（首次需要执行）</p><pre><code>cd  /opt/bigdata/hadoop-3.1.4/ #首先进入hadoop根目录
bin/hdfs namenode -format #执行格式化命令
</code></pre><p>然后，启动 hdfs 集群</p><pre><code>sbin/start-dfs.sh
</code></pre><p>再启动 yarn 集群</p><pre><code>sbin/start-yarn.sh
</code></pre><p>最后，启动 historyserver，查看历史完成的任务</p><pre><code>sbin/mr-jobhistory-daemon.sh start historyserver
</code></pre><p>使用 jps 查看进程</p><pre><code>[root@master~]#jps
</code></pre><h2 id="7-查看可视化界面验证启动是否成功"><a class="anchor" href="#7-查看可视化界面验证启动是否成功">#</a> 7、查看可视化界面，验证启动是否成功</h2><p>1、首先访问：<span class="exturl" data-url="aHR0cDovLzE5Mi4xNjguMi4xMjg6NTAwNzAvZXhwbG9yZXIuaHRtbCMvJUVGJUJDJThDJUU3JTk0JUE4JUU2JTlEJUE1JUU2JTlGJUE1JUU3JTlDJThCaGRmcw==">http://192.168.2.128:50070/explorer.html#/，用来查看 hdfs</span></p><p>2、然后访问：<span class="exturl" data-url="aHR0cDovLzE5Mi4xNjguMi4xMjg6ODA4OC9jbHVzdGVyJUVGJUJDJThDJUU2JTlGJUE1JUU3JTlDJThCeWFybiVFOSU5QiU4NiVFNyVCRSVBNA==">http://192.168.2.128:8088/cluster，查看 yarn 集群</span></p><p>3、最后访问：<span class="exturl" data-url="aHR0cDovLzE5Mi4xNjguMi4xMjg6MTk4ODgvam9iaGlzdG9yeSVFRiVCQyU4QyVFNiU5RiVBNSVFNyU5QyU4QiVFNSU4RSU4NiVFNSU4RiVCMiVFNSVBRSU4QyVFNiU4OCU5MCVFNyU5QSU4NCVFNCVCQiVCQiVFNSU4QSVBMQ==">http://192.168.2.128:19888/jobhistory，查看历史完成的任务</span></p><p>如果都有页面表示搭建成功！</p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2020-11-21 18:49:15" itemprop="dateModified" datetime="2020-11-21T18:49:15+08:00">2020-11-21</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Andrésen 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Andrésen 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="Andrésen 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Andrésen <i class="ic i-at"><em>@</em></i>Andrésen</li><li class="link"><strong>本文链接：</strong> <a href="https://methiony.work/2020/11/21/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">https://methiony.work/2020/11/21/Hadoop分布式集群搭建/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2020/11/21/Zookeeper%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva3.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclimtf7dj20zk0m8qav.jpg" title="Zookeeper基础知识"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i></span><h3>Zookeeper基础知识</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">1.</span> <span class="toc-text">Hadoop 分布式集群搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">1.1.</span> <span class="toc-text">1、环境准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.</span> <span class="toc-text">2、环境配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%89%E8%A3%85jdk"><span class="toc-number">1.3.</span> <span class="toc-text">3、安装 JDK</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%89%E8%A3%85hadoop"><span class="toc-number">1.4.</span> <span class="toc-text">4、安装 hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%88%86%E5%8F%91jdk-hadoop%E5%88%B0node01-node02"><span class="toc-number">1.5.</span> <span class="toc-text">5、分发 jdk、hadoop 到 node01、node02</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">1.6.</span> <span class="toc-text">6、启动集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%9F%A5%E7%9C%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2%E9%AA%8C%E8%AF%81%E5%90%AF%E5%8A%A8%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F"><span class="toc-number">1.7.</span> <span class="toc-text">7、查看可视化界面，验证启动是否成功</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Andrésen" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Andrésen</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">5</span> <span class="name">文章</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lvdXJuYW1l" title="https:&#x2F;&#x2F;github.com&#x2F;yourname"><i class="ic i-github"></i></span> <span class="exturl item twitter" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;twitter.com&#x2F;yourname"><i class="ic i-twitter"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yourname"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPXlvdXJpZA==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;yourid"><i class="ic i-cloud-music"></i></span> <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20veW91cm5hbWU=" title="https:&#x2F;&#x2F;weibo.com&#x2F;yourname"><i class="ic i-weibo"></i></span> <span class="exturl item about" data-url="aHR0cHM6Ly9hYm91dC5tZS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;about.me&#x2F;yourname"><i class="ic i-address-card"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/2020/11/21/Zookeeper%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="Zookeeper基础知识">Zookeeper基础知识</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2020/11/06/hello-world/" title="Hello World">Hello World</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2020/11/21/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2020/11/15/MySQL-0/" title="MySQL">MySQL</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2020/11/19/Hbase/" title="Hbase">Hbase</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Andrésen @ Methiony Shoka</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">12k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">11 分钟</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2020/11/21/Hadoop分布式集群搭建/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.4"></script></body></html><!-- rebuild by hrmmi -->